# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U66zS7C5Y4gTYrl_3caUQsxlyIulmg6p
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset from CSV
from google.colab import files

uploaded = files.upload()

import io
file_name = list(uploaded.keys())[0]
df = data = pd.read_csv(io.BytesIO(uploaded[file_name]))


# Exploratory Data Analysis (EDA)
# Let's take a quick look at the first few rows of the dataset
print(df.head())

# Summary statistics of the dataset
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Correlation matrix to understand feature relationships
# Check the data types of each column to identify non-numeric columns
print(df.dtypes)

# Convert categorical columns to numeric, if any
# Check the data types of each column to identify non-numeric columns
print("Data types before encoding:\n", df.dtypes)

# Loop through each column to identify and convert non-numeric columns
for col in df.columns:
    if df[col].dtype == 'object':
        unique_values = df[col].unique()
        print(f"Column '{col}' has non-numeric values: {unique_values}")

        # If the column has 'yes'/'no' values, map them to 1 and 0
        if set(unique_values) == {'yes', 'no'}:
            df[col] = df[col].map({'yes': 1, 'no': 0})
        else:
            # For columns with other string values, apply one-hot encoding
            df = pd.get_dummies(df, columns=[col], drop_first=True)

# Verify if all columns are now numeric
print("Data types after encoding:\n", df.dtypes)

# Calculate the correlation matrix
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()


# Preprocessing: Selecting features and target variable
X = df[['bedrooms', 'bathrooms', 'mainroad', 'airconditioning']]
y = df['price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Building the Linear Regression Model
model = LinearRegression()

# Fitting the model on the training data
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Mean Squared Error and R-squared for model evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Predictions and Visualization
# To visualize the predictions against actual prices, we'll use a scatter plot
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual Prices vs. Predicted Prices")
plt.show()

# We can also create a residual plot to check the model's performance
residuals = y_test - y_pred
plt.scatter(y_test, residuals)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel("Actual Prices")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

# Lastly, let's use the trained model to make predictions on new data and visualize the results
new_data = [[3, 2, 1500, 4000, 1, 0, 0, 3]]
predicted_price = model.predict(new_data)

print("Predicted Price:", predicted_price[0])